{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank notebook for reproducibility\n",
    "\n",
    "This notebook contains a full walkthrough on how to use the code on the Reddit TIFU dataset. The output is done on a 30 post subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data structures\n",
    "import pandas as pd # Data/tabular structures\n",
    "import numpy as np # Linear algebra\n",
    "\n",
    "## Natural language processing\n",
    "import nltk # Natural language toolkit\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('punkt') # one time execution\n",
    "\n",
    "## Utilities\n",
    "\n",
    "import re # RegEx\n",
    "import math \n",
    "from tqdm import tqdm # Progress bars\n",
    "\n",
    "## Similarity calculation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx # Graph construction\n",
    "\n",
    "## Evaluation notebook\n",
    "\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Initializations\n",
    "\n",
    "Shoutout to the author of: https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/ for inspiration and example of a TextRank implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to import Glove embeddings\n",
    "\n",
    "def create_word_embeddings(embedding_path):\n",
    "\n",
    "    word_embedding_dict = {} # Create return object\n",
    "    \n",
    "    f = open(embedding_path, encoding='utf-8') # Open embedding file\n",
    "\n",
    "    # Process file\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embedding_dict[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Word embeddings succesfully extracted!\")\n",
    "    \n",
    "    return(word_embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to preprocess data\n",
    "\n",
    "def remove_stopwords(sen): # Remove stopwords using NLTK stopwords\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "def pre_process_single_doc(doc):\n",
    "    \n",
    "    sentences = [] # Create empty temporary list\n",
    "    \n",
    "    for s in doc['documents']:\n",
    "        sentences.append(sent_tokenize(s))\n",
    "    \n",
    "    sentences = [y for x in sentences for y in x] # flatten list\n",
    "    \n",
    "    sentences = list(pd.Series(sentences)) # Transform to list\n",
    "    \n",
    "    # Replace HTML/Markup\n",
    "\n",
    "    clean_sentences = list(map(lambda x: x.replace('\\n','').replace('\"b','').replace('\\\\n','').replace(\"'b\",'').replace('b\"','').replace(\"b'\",''),sentences))\n",
    "    \n",
    "    # Replace URL's\n",
    "\n",
    "    clean_sentences = [re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])','URL',i) for i in clean_sentences]\n",
    "    \n",
    "    # Replace subreddit indicator\n",
    "    \n",
    "    clean_sentences = list(map(lambda x: x.replace('r/','subreddit '),clean_sentences))\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "\n",
    "    clean_sentences = [re.sub(r\"[^a-zA-Z0-9]+\",\" \",i) for i in clean_sentences]\n",
    "\n",
    "    # Lowercase everything \n",
    "\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    return(sentences, clean_sentences) # Return preprocessed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to transform sentence object to a word embedding vector\n",
    "\n",
    "def create_sentence_vectors(preprocessed_doc):\n",
    "\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    # Use embedding document to create embedding vector\n",
    "    for i in preprocessed_doc:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    \n",
    "    return(sentence_vectors) # Return the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get longest common substring \n",
    "\n",
    "def calculate_lcs(string1, string2):\n",
    "    lcs = 0;\n",
    "    \n",
    "    # Calculate the LCS using two strings (sentences)\n",
    "    for a in range(len(string1)):\n",
    "         for b in range(len(string2)):\n",
    "            k = 0;\n",
    "            while ((a + k) < len(string1) and (b + k) < len(string2)\n",
    "        and string1[a + k] == string2[b + k]):\n",
    "                k = k + 1;\n",
    "\n",
    "            lcs = max(lcs, k);\n",
    "    return lcs; # Return LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Similarity Matrix based on Cosine Similarity\n",
    "\n",
    "def create_similarity_matrix_cosine_vector(sen_vec,sen_list):\n",
    "\n",
    "    # Create similarity matrix\n",
    "\n",
    "    sim_mat = np.zeros([len(sen_list), len(sen_list)]) # Initialize similarity matrix\n",
    "\n",
    "    for i in range(len(sen_list)): # Pair wise similarity calculation\n",
    "        for j in range(len(sen_list)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sen_vec[i].reshape(1,100), sen_vec[j].reshape(1,100))[0,0] # Append to matrix\n",
    "    \n",
    "    return(sim_mat) # Return matrix object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Similarity Matrix for TF-IDF\n",
    "\n",
    "def create_similarity_matrix_TFIDF(sen_list):\n",
    "\n",
    "    # Create similarity matrix\n",
    "\n",
    "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\") # Use library for pairwise TFIDF calculation\n",
    "    tfidf = vect.fit_transform(sen_list)     \n",
    "    pairwise_similarity = tfidf * tfidf.T # Create matrix\n",
    "    sim_mat = pairwise_similarity.toarray() \n",
    "    np.fill_diagonal(sim_mat, 0) # Zero out self similarity\n",
    "\n",
    "    return(sim_mat) # Return matrix object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Similarity Matrix based on LCS Similarity\n",
    "\n",
    "def create_similarity_matrix_lcs(sen_list):\n",
    "\n",
    "    # Create similarity matrix\n",
    "\n",
    "    sim_mat = np.zeros([len(sen_list), len(sen_list)]) # Initialize empty matrix\n",
    "\n",
    "    for i in range(len(sen_list)): # Pairwise\n",
    "        for j in range(len(sen_list)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = calculate_lcs(sen_list[i], sen_list[j]) # Append LCS\n",
    "    \n",
    "    return(sim_mat) # Return matrix object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Graph from Similarity Matrix\n",
    "\n",
    "def create_graph_and_rank(sim_mat,sen_list,plot_graph=False):\n",
    "    \n",
    "    # Create graph\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "\n",
    "    # Call Pagerank\n",
    "\n",
    "    scores = nx.pagerank_numpy(nx_graph)\n",
    "    \n",
    "    if plot_graph == True:\n",
    "        # Plot the graph\n",
    "        nx.draw(nx_graph)\n",
    "    \n",
    "    # Rank Sentences Based on PageRank Algorithm\n",
    "    \n",
    "    return(sorted(((scores[i],s) for i,s in enumerate(sen_list)), reverse=True)) # Return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top sentences as the summary\n",
    "\n",
    "def generate_summary(sen_ranks, sen_list):\n",
    "\n",
    "    # Use 20% of the length of the original post\n",
    "\n",
    "    sum_size = math.ceil(int(0.2 * len(sen_list)))\n",
    "    \n",
    "    if sum_size < 1: # Fail-safe\n",
    "        sum_size = 1\n",
    "\n",
    "    order_dict = dict()\n",
    "    \n",
    "    # Extract Top-N and re-order\n",
    "    \n",
    "    for i in range(sum_size):\n",
    "        order_dict.update({sen_ranks[i][1]: sen_list.index(sen_ranks[i][1])})\n",
    "\n",
    "    sorted_values = sorted(order_dict.values()) # Sort the values\n",
    "    sorted_dict = {}\n",
    "\n",
    "    for i in sorted_values:\n",
    "        for k in order_dict.keys():\n",
    "            if order_dict[k] == i:\n",
    "                sorted_dict[k] = order_dict[k]\n",
    "                break\n",
    "                \n",
    "    # Conduct some pre-processing on output to increase readibility\n",
    "\n",
    "    clean_sentences = list(map(lambda x: x.replace('\\n','').replace('\"b','').replace('\\\\n','').replace(\"'b\",'').replace('b\"','').replace(\"b'\",''),sorted_dict))\n",
    "\n",
    "    clean_sentences = [re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])','URL',i) for i in clean_sentences]\n",
    "\n",
    "    clean_sentences = list(map(lambda x: x.replace('r/','subreddit '),clean_sentences))\n",
    "\n",
    "    clean_sentences = [re.sub(r\"[^a-zA-Z0-9%',.-]+\",\" \",i) for i in clean_sentences]\n",
    "    \n",
    "    return(\" \".join(clean_sentences)) # Return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct some pre-processing on the source data to remove noise\n",
    "\n",
    "def clean_document(column):\n",
    "    column_new = list(map(lambda x: x.replace('\\n','').replace('\"b','').replace('\\\\n','').replace(\"'b\",'').replace('b\"','').replace(\"b'\",''),column))\n",
    "    \n",
    "    column_new = [re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])','URL',i) for i in column_new]\n",
    "    \n",
    "    column_new = [re.sub(r\"[^a-zA-Z0-9%!?:;,.-]+\",\" \",i) for i in column_new]\n",
    "                                                                                                                   \n",
    "    column_output = list(map(lambda x: x.replace('r/','subreddit '),column_new))\n",
    "\n",
    "    return(column_output) # Return processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifu_dataset = pd.read_csv(\"tifu-dataset-snippet.csv\") # Example in the GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   index         30 non-null     int64  \n",
      " 1   documents     30 non-null     object \n",
      " 2   num_comments  30 non-null     int64  \n",
      " 3   score         30 non-null     int64  \n",
      " 4   title         30 non-null     object \n",
      " 5   tldr          30 non-null     object \n",
      " 6   ups           30 non-null     int64  \n",
      " 7   upvote_ratio  30 non-null     float64\n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "tifu_dataset.info() # Inspect the columns/ shape (30 subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>documents</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>tldr</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5900</td>\n",
       "      <td>b\"this happened to me yesterday and i'm still ...</td>\n",
       "      <td>2715</td>\n",
       "      <td>85249</td>\n",
       "      <td>b'having my reddit history revealed by jimmy k...</td>\n",
       "      <td>b\"jimmy kimmel had reddit co-founder alexis oh...</td>\n",
       "      <td>85249</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34594</td>\n",
       "      <td>b'edit: i got an ama thread now. help me: \\n\\n...</td>\n",
       "      <td>3825</td>\n",
       "      <td>81547</td>\n",
       "      <td>b'cumming into a coconut'</td>\n",
       "      <td>b\"don't fuck coconuts.\"</td>\n",
       "      <td>81547</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37412</td>\n",
       "      <td>b'recently, i traveled to denver, colorado wit...</td>\n",
       "      <td>4262</td>\n",
       "      <td>80916</td>\n",
       "      <td>b\"stuffing my face with edibles before dinner ...</td>\n",
       "      <td>b'ate way too many edibles on a trip and wigge...</td>\n",
       "      <td>80916</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37333</td>\n",
       "      <td>b'this happened sunday night.\\n\\n\\n\\nmy oldest...</td>\n",
       "      <td>1166</td>\n",
       "      <td>49877</td>\n",
       "      <td>b'not telling my wife our son was coming home'</td>\n",
       "      <td>b\"didn't tell wife that marine son was coming ...</td>\n",
       "      <td>49877</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4506</td>\n",
       "      <td>b'i\\'m in my third year of university taking e...</td>\n",
       "      <td>2466</td>\n",
       "      <td>48085</td>\n",
       "      <td>b'sitting in the wrong class for an entire mon...</td>\n",
       "      <td>b'i was in the wrong econ class for an entire ...</td>\n",
       "      <td>48085</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                          documents  num_comments  \\\n",
       "0   5900  b\"this happened to me yesterday and i'm still ...          2715   \n",
       "1  34594  b'edit: i got an ama thread now. help me: \\n\\n...          3825   \n",
       "2  37412  b'recently, i traveled to denver, colorado wit...          4262   \n",
       "3  37333  b'this happened sunday night.\\n\\n\\n\\nmy oldest...          1166   \n",
       "4   4506  b'i\\'m in my third year of university taking e...          2466   \n",
       "\n",
       "   score                                              title  \\\n",
       "0  85249  b'having my reddit history revealed by jimmy k...   \n",
       "1  81547                          b'cumming into a coconut'   \n",
       "2  80916  b\"stuffing my face with edibles before dinner ...   \n",
       "3  49877     b'not telling my wife our son was coming home'   \n",
       "4  48085  b'sitting in the wrong class for an entire mon...   \n",
       "\n",
       "                                                tldr    ups  upvote_ratio  \n",
       "0  b\"jimmy kimmel had reddit co-founder alexis oh...  85249          0.88  \n",
       "1                            b\"don't fuck coconuts.\"  81547          0.90  \n",
       "2  b'ate way too many edibles on a trip and wigge...  80916          0.88  \n",
       "3  b\"didn't tell wife that marine son was coming ...  49877          0.87  \n",
       "4  b'i was in the wrong econ class for an entire ...  48085          0.90  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tifu_dataset.head() # Visually inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings succesfully extracted!\n"
     ]
    }
   ],
   "source": [
    "# Read in word embeddings (downloadable at https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "word_embeddings = create_word_embeddings('glove.6b/glove.6B.100d.txt')\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the TextRank algorithm (refer to the pdf in the github for a written explanation)\n",
    "\n",
    "def pipeline_textrank_summarization(doc,sim_measure): # Sim measure should be either cosine, lcs or TFIDF\n",
    "\n",
    "    if sim_measure == \"cosine\":\n",
    "        \n",
    "        sentence_list, pre_processed_sentences = pre_process_single_doc(doc)\n",
    "\n",
    "        sentence_vector = create_sentence_vectors(pre_processed_sentences)\n",
    "\n",
    "        similarity_matrix = create_similarity_matrix_cosine_vector(sentence_vector,sentence_list)\n",
    "\n",
    "        ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)\n",
    "\n",
    "        summary = generate_summary(ranking_scores,sentence_list)\n",
    "                \n",
    "    if sim_measure == \"lcs\":\n",
    "        \n",
    "        sentence_list, pre_processed_sentences = pre_process_single_doc(doc)\n",
    "        \n",
    "        similarity_matrix = create_similarity_matrix_lcs(pre_processed_sentences)\n",
    "\n",
    "        ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)\n",
    "\n",
    "        summary = generate_summary(ranking_scores,sentence_list)\n",
    "        \n",
    "    if sim_measure == \"tfidf\":\n",
    "        \n",
    "        sentence_list, pre_processed_sentences = pre_process_single_doc(doc)\n",
    "        \n",
    "        similarity_matrix = create_similarity_matrix_TFIDF(pre_processed_sentences)\n",
    "\n",
    "        ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)\n",
    "\n",
    "        summary = generate_summary(ranking_scores,sentence_list)\n",
    "                \n",
    "    return(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One post example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TEXT______________\n",
      "b'this happened only minutes ago.\\n\\n\\nthe graphics card in my old ps2 decided it wanted to give up on me recently, so i decided to replace it when i had a little extra cash. i was out browsing different sites like craigslist and the like, when i stumbled upon the ps2 mentioned in the title. it looked like a great deal at the time. $25 to buy it from this guy, whereas a secondhand store in town was selling them for around $45 to $60.\\n\\n\\nat the time, this seemed like a no-brainer.\\n\\n\\nnow, i should preface this by saying that i have a strange faith in the honesty of others. benefit of the doubt and all that noise. after all, the car i drive now is one i bought from a guy on the internet, and it runs great for something that is 27 years old. why should this be any different?\\n\\n\\nstarting to sound like a mistake yet?\\n\\n\\nif the answer is \"no,\" then have no fear. that is almost certainly about to change. the model the seller advertised on letgo was one of the larger black brick models. like, the first generation ones. when i finally met the seller, we had a brief handshake and exchange. when he showed me the goods, it was actually one of the slim and silver ps2\\'s from later in the console\\'s production run. he told me he had already sold the larger one.\\n\\n\\nmy first instinct, as many logical redditors would tell me, is that i should have walked away when i saw i was being sold something that was improperly advertised. unfortunately for me (and i guess, fortunately for the rest of you reading this), i was not in the most logical mood. it was kind of a shitty drive to a pretty sketchy part of town, and it had already been a long morning. i just wanted to get this thing and go home.\\n\\n\\nback at the ole ranch, i hooked up the console, slapped in kingdom hearts ii, and got ready to enjoy the rest of my day. i\\'m off work and done with class, what else would i do with my evening, right? i was pretty happy with my life until i realized i had been waiting for five or so minutes and the disc wasn\\'t reading. strange, but not altogether discouraging. i slapped in another game that i new for sure was working. didn\\'t read. i repeated this several times with discs i knew would work; same story.\\n\\n\\nnow i\\'m starting to get slightly pissed. i\\'m sure you can see where this next part is going.\\n\\n\\ni open up the app to message the seller and let him know he had sold me a defective console. not so much to my surprise, he had blocked me. i\\'ve now got no way to contact him. of course not, he just took me for a fucking ride. all i could do at that point was report him and live in my shame. that, or i could get onto youtube and try to find a fix for a disc-read error. i picked the latter option and went on my merry way. after a video about a quick mod i could make to the system, i was feeling pretty confident that i was going to get the last laugh on this one.\\n\\n\\noh golly gee fuck was i wrong.\\n\\n\\nnow at this point, i begin to tumble from my perch of the high and mighty, and start snowballing right down that tall fucking mountain. i manage to get the appropriate screws out and pry that bad boy open. it\\'s putting up a little more of a fight than i saw in the videos, but why wouldn\\'t it? it was never meant to be opened this way in the first place. with a little elbow grease, it starts to give. i\\'m also starting to spot a bit of rust in one of the memory card slots. a bit annoying, but the other one was working a few minutes ago so i can live with that. the further i get it open, the more rust i see on some more critical pieces. i\\'m sweating again, but holding out hope. finally, the plastic casing comes free, and i experience the joy of a man that is working with his own two hands to solve a problem.\\n\\n\\ni experience this joy for maybe a second and a half.\\n\\n\\nyou see, after prying the top casing off of the console, the bottom half sort of clattered back down onto the desk. the first thing i notice is that the inside is covered in rust. the second thing i notice is that some of the rust is moving.\\n\\n\\noh, that\\'s not all rust.\\n\\n\\n*ohsonofabitchthosearefuck-motheringcockroaches.*\\n\\n\\ndo you know that scene in raiders of the lost ark where indiana jones is like, \"snakes. why did it have to be snakes?\" that was me. except with cockroaches. they are the one insect that fill me with more disgust than anything else. and now they\\'re crawling on my hands. and these are the quick little tiny ones that don\\'t give two fucks about whether you\\'ve stepped on them or not. i threw that whole console on the ground post haste, which only made them more riled up.\\n\\n\\nthey\\'re getting everywhere now. i\\'m yelling for help to my roommates and stomping the ground like a fucking mad man in a fit of terrified panic. you would think that something caught fire with the way i was yelling (which is what my roommate said he thought had happened), but no, just cockroaches. tiny, $25 freaks of nature. eventually i managed to eradicate most of the insect horde with stomping and shouting alone, at which point i grabbed a can of raid from the laundry room and finished the job.\\n\\n\\nat least, i *hope* i finished the job. i\\'m fairly certain there are at least two of those little fuckers still running around under the bed, but at this point i\\'m a little too emotionally drained to care. i also get the extreme pleasure of wallowing in the fact that i *basically* just paid $25 to get covered in my least favorite living thing on the planet. jesus, it\\'s only 3 in the afternoon and i already need a stiff drink.\\n\\n\\n \\n\\nedit: hi, reddit! i just wanted to say many thanks for the gold and all the offers for cockroach-free playstations!  warms my heart, and i\\'m super happy you guys enjoyed my writing as well. happy redditing!'\n",
      "_____________TLDR______________\n",
      "b'i bought a used ps2 online, opened it up to fix it, and got covered in cockroaches.'\n"
     ]
    }
   ],
   "source": [
    "sample_post = tifu_dataset[8:9] # Select one document\n",
    "\n",
    "# Print post and TLDR\n",
    "\n",
    "text = sample_post['documents'][8]\n",
    "print(\"_____________TEXT______________\")\n",
    "print(text)\n",
    "\n",
    "tldr = sample_post['tldr'][8]\n",
    "print(\"_____________TLDR______________\")\n",
    "print(tldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______ LCS method _______\n",
      " 25 to buy it from this guy, whereas a secondhand store in town was selling them for around 45 to 60.at the time, this seemed like a no-brainer.now, i should preface this by saying that i have a strange faith in the honesty of others. he told me he had already sold the larger one.my first instinct, as many logical redditors would tell me, is that i should have walked away when i saw i was being sold something that was improperly advertised. i just wanted to get this thing and go home.back at the ole ranch, i hooked up the console, slapped in kingdom hearts ii, and got ready to enjoy the rest of my day. i repeated this several times with discs i knew would work same story.now i 'm starting to get slightly pissed. i 'm sure you can see where this next part is going.i open up the app to message the seller and let him know he had sold me a defective console. after a video about a quick mod i could make to the system, i was feeling pretty confident that i was going to get the last laugh on this one.oh golly gee fuck was i wrong.now at this point, i begin to tumble from my perch of the high and mighty, and start snowballing right down that tall fucking mountain. finally, the plastic casing comes free, and i experience the joy of a man that is working with his own two hands to solve a problem.i experience this joy for maybe a second and a half.you see, after prying the top casing off of the console, the bottom half sort of clattered back down onto the desk. the second thing i notice is that some of the rust is moving.oh, that 's not all rust. ohsonofabitchthosearefuck-motheringcockroaches. i 'm yelling for help to my roommates and stomping the ground like a fucking mad man in a fit of terrified panic. you would think that something caught fire with the way i was yelling which is what my roommate said he thought had happened , but no, just cockroaches. i 'm fairly certain there are at least two of those little fuckers still running around under the bed, but at this point i 'm a little too emotionally drained to care.\n",
      "_______ cosine method _______\n",
      "it looked like a great deal at the time.  25 to buy it from this guy, whereas a secondhand store in town was selling them for around 45 to 60.at the time, this seemed like a no-brainer.now, i should preface this by saying that i have a strange faith in the honesty of others. why should this be any different starting to sound like a mistake yet if the answer is no, then have no fear. he told me he had already sold the larger one.my first instinct, as many logical redditors would tell me, is that i should have walked away when i saw i was being sold something that was improperly advertised. i 'm off work and done with class, what else would i do with my evening, right  i repeated this several times with discs i knew would work same story.now i 'm starting to get slightly pissed. after a video about a quick mod i could make to the system, i was feeling pretty confident that i was going to get the last laugh on this one.oh golly gee fuck was i wrong.now at this point, i begin to tumble from my perch of the high and mighty, and start snowballing right down that tall fucking mountain. a bit annoying, but the other one was working a few minutes ago so i can live with that. finally, the plastic casing comes free, and i experience the joy of a man that is working with his own two hands to solve a problem.i experience this joy for maybe a second and a half.you see, after prying the top casing off of the console, the bottom half sort of clattered back down onto the desk. i 'm fairly certain there are at least two of those little fuckers still running around under the bed, but at this point i 'm a little too emotionally drained to care. i also get the extreme pleasure of wallowing in the fact that i basically just paid 25 to get covered in my least favorite living thing on the planet.\n",
      "_______ TFIDF method _______\n",
      "this happened only minutes ago.the graphics card in my old ps2 decided it wanted to give up on me recently, so i decided to replace it when i had a little extra cash. i was out browsing different sites like craigslist and the like, when i stumbled upon the ps2 mentioned in the title.  25 to buy it from this guy, whereas a secondhand store in town was selling them for around 45 to 60.at the time, this seemed like a no-brainer.now, i should preface this by saying that i have a strange faith in the honesty of others. like, the first generation ones. i was pretty happy with my life until i realized i had been waiting for five or so minutes and the disc wasn 't reading. i 'm sure you can see where this next part is going.i open up the app to message the seller and let him know he had sold me a defective console. the first thing i notice is that the inside is covered in rust. the second thing i notice is that some of the rust is moving.oh, that 's not all rust. ohsonofabitchthosearefuck-motheringcockroaches. and these are the quick little tiny ones that don 't give two fucks about whether you 've stepped on them or not. i 'm yelling for help to my roommates and stomping the ground like a fucking mad man in a fit of terrified panic. you would think that something caught fire with the way i was yelling which is what my roommate said he thought had happened , but no, just cockroaches.\n"
     ]
    }
   ],
   "source": [
    "# Generate and print 3 summaries of the sample\n",
    "\n",
    "print(\"_______ LCS method _______\")\n",
    "\n",
    "summary_lcs = pipeline_textrank_summarization(sample_post,\"lcs\")\n",
    "print(summary_lcs)\n",
    "\n",
    "print(\"_______ cosine method _______\")\n",
    "\n",
    "summary_cosine = pipeline_textrank_summarization(sample_post,\"cosine\")\n",
    "print(summary_cosine)\n",
    "\n",
    "print(\"_______ TFIDF method _______\")\n",
    "\n",
    "summary_tfidf = pipeline_textrank_summarization(sample_post,\"tfidf\")\n",
    "print(summary_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.5, 'p': 0.05555555555555555, 'f': 0.09999999820000001},\n",
       "  'rouge-2': {'r': 0.058823529411764705,\n",
       "   'p': 0.0038910505836575876,\n",
       "   'f': 0.007299268909105626},\n",
       "  'rouge-l': {'r': 0.5, 'p': 0.05555555555555555, 'f': 0.09999999820000001}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROUGE scores for sample\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge.get_scores(summary_tfidf, tldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset summarization\n",
    "\n",
    "The summarization of the sample should be relatively quick, for reference the full dataset took ~13 hours. Your experience may vary on hardware setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:16<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Lists to save summaries\n",
    "\n",
    "summary_list_LCS = []\n",
    "summary_list_Cosine = []\n",
    "summary_list_TFIDF = []\n",
    "\n",
    "for index, row in tqdm(tifu_dataset.iterrows(), total=tifu_dataset.shape[0]): # Loop over entire collection\n",
    "    curr_post = tifu_dataset[index:index+1] # Select one document\n",
    "    \n",
    "    try: # Catching errors so that the process can't fail midway one one sample\n",
    "        summary_list_Cosine.append(pipeline_textrank_summarization(curr_post,\"cosine\")) # Glove cosine similarity\n",
    "    except Exception:\n",
    "        summary_list_Cosine.append(\"error\")\n",
    "        \n",
    "    try:\n",
    "        summary_list_LCS.append(pipeline_textrank_summarization(curr_post,\"lcs\")) # LCS\n",
    "    except Exception:\n",
    "        summary_list_LCS.append(\"error\")\n",
    "                \n",
    "    try:\n",
    "        summary_list_TFIDF.append(pipeline_textrank_summarization(curr_post,\"tfidf\")) # TF-IDF\n",
    "    except Exception:\n",
    "        summary_list_TFIDF.append(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reference TLDR as a list\n",
    "\n",
    "reference_list = list(tifu_dataset['tldr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append new summaries to the dataframe\n",
    "\n",
    "tifu_dataset[\"LCS\"] = summary_list_LCS\n",
    "tifu_dataset[\"COSINE\"] = summary_list_Cosine\n",
    "tifu_dataset[\"TFIDF\"] = summary_list_TFIDF\n",
    "\n",
    "# Clean a few columns of noise\n",
    "\n",
    "documents = tifu_dataset.documents\n",
    "title = tifu_dataset.title\n",
    "tldr = tifu_dataset.tldr\n",
    "\n",
    "tifu_dataset[\"documents\"] = clean_document(documents)\n",
    "tifu_dataset[\"title\"] = clean_document(title)\n",
    "tifu_dataset[\"tldr\"] = clean_document(tldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>documents</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>tldr</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>LCS</th>\n",
       "      <th>COSINE</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5900</td>\n",
       "      <td>this happened to me yesterday and i m still ba...</td>\n",
       "      <td>2715</td>\n",
       "      <td>85249</td>\n",
       "      <td>having my reddit history revealed by jimmy kim...</td>\n",
       "      <td>jimmy kimmel had reddit co-founder alexis ohan...</td>\n",
       "      <td>85249</td>\n",
       "      <td>0.88</td>\n",
       "      <td>this alone was crazy for me because i never ex...</td>\n",
       "      <td>however it ended up on the front page with muc...</td>\n",
       "      <td>i was mind blown over this.they proceeded talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34594</td>\n",
       "      <td>edit: i got an ama thread now. help me: URL ti...</td>\n",
       "      <td>3825</td>\n",
       "      <td>81547</td>\n",
       "      <td>cumming into a coconut</td>\n",
       "      <td>don t fuck coconuts.</td>\n",
       "      <td>81547</td>\n",
       "      <td>0.90</td>\n",
       "      <td>i end up grabbing the coconut drill and throug...</td>\n",
       "      <td>one day i hear that my mother is going to be o...</td>\n",
       "      <td>edit i got an ama thread now. horny me decides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37412</td>\n",
       "      <td>recently, i traveled to denver, colorado with ...</td>\n",
       "      <td>4262</td>\n",
       "      <td>80916</td>\n",
       "      <td>stuffing my face with edibles before dinner wi...</td>\n",
       "      <td>ate way too many edibles on a trip and wigged ...</td>\n",
       "      <td>80916</td>\n",
       "      <td>0.88</td>\n",
       "      <td>what could possibly go wrong so the first thin...</td>\n",
       "      <td>as a result, before leaving, i begged my wife ...</td>\n",
       "      <td>recently, i traveled to denver, colorado with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37333</td>\n",
       "      <td>this happened sunday night.my oldest son is in...</td>\n",
       "      <td>1166</td>\n",
       "      <td>49877</td>\n",
       "      <td>not telling my wife our son was coming home</td>\n",
       "      <td>didn t tell wife that marine son was coming ho...</td>\n",
       "      <td>49877</td>\n",
       "      <td>0.87</td>\n",
       "      <td>i get to the airport, and actually watched the...</td>\n",
       "      <td>now the whole time ive been texting my wife sa...</td>\n",
       "      <td>now the whole time ive been texting my wife sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4506</td>\n",
       "      <td>i m in my third year of university taking engi...</td>\n",
       "      <td>2466</td>\n",
       "      <td>48085</td>\n",
       "      <td>sitting in the wrong class for an entire month...</td>\n",
       "      <td>i was in the wrong econ class for an entire mo...</td>\n",
       "      <td>48085</td>\n",
       "      <td>0.90</td>\n",
       "      <td>i 'm in my third year of university taking eng...</td>\n",
       "      <td>i 'm in my third year of university taking eng...</td>\n",
       "      <td>i came to class. so the class before the exam,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                          documents  num_comments  \\\n",
       "0   5900  this happened to me yesterday and i m still ba...          2715   \n",
       "1  34594  edit: i got an ama thread now. help me: URL ti...          3825   \n",
       "2  37412  recently, i traveled to denver, colorado with ...          4262   \n",
       "3  37333  this happened sunday night.my oldest son is in...          1166   \n",
       "4   4506  i m in my third year of university taking engi...          2466   \n",
       "\n",
       "   score                                              title  \\\n",
       "0  85249  having my reddit history revealed by jimmy kim...   \n",
       "1  81547                            cumming into a coconut    \n",
       "2  80916  stuffing my face with edibles before dinner wi...   \n",
       "3  49877       not telling my wife our son was coming home    \n",
       "4  48085  sitting in the wrong class for an entire month...   \n",
       "\n",
       "                                                tldr    ups  upvote_ratio  \\\n",
       "0  jimmy kimmel had reddit co-founder alexis ohan...  85249          0.88   \n",
       "1                              don t fuck coconuts.   81547          0.90   \n",
       "2  ate way too many edibles on a trip and wigged ...  80916          0.88   \n",
       "3  didn t tell wife that marine son was coming ho...  49877          0.87   \n",
       "4  i was in the wrong econ class for an entire mo...  48085          0.90   \n",
       "\n",
       "                                                 LCS  \\\n",
       "0  this alone was crazy for me because i never ex...   \n",
       "1  i end up grabbing the coconut drill and throug...   \n",
       "2  what could possibly go wrong so the first thin...   \n",
       "3  i get to the airport, and actually watched the...   \n",
       "4  i 'm in my third year of university taking eng...   \n",
       "\n",
       "                                              COSINE  \\\n",
       "0  however it ended up on the front page with muc...   \n",
       "1  one day i hear that my mother is going to be o...   \n",
       "2  as a result, before leaving, i begged my wife ...   \n",
       "3  now the whole time ive been texting my wife sa...   \n",
       "4  i 'm in my third year of university taking eng...   \n",
       "\n",
       "                                               TFIDF  \n",
       "0  i was mind blown over this.they proceeded talk...  \n",
       "1  edit i got an ama thread now. horny me decides...  \n",
       "2  recently, i traveled to denver, colorado with ...  \n",
       "3  now the whole time ive been texting my wife sa...  \n",
       "4  i came to class. so the class before the exam,...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tifu_dataset.head() # New dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This is normally done in a seperate notebook but for the sake of having all sources at one place here it is demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.355054805272828,\n",
       "  'p': 0.07393664754696957,\n",
       "  'f': 0.11237401721113938},\n",
       " 'rouge-2': {'r': 0.07395258537429861,\n",
       "  'p': 0.009412406316520816,\n",
       "  'f': 0.015862560120891957},\n",
       " 'rouge-l': {'r': 0.3116353387853391,\n",
       "  'p': 0.06040301101219256,\n",
       "  'f': 0.09446451371731314}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summary_list_LCS, reference_list,avg=True) # LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.3373625388625309,\n",
       "  'p': 0.07447375794115715,\n",
       "  'f': 0.11375380781620383},\n",
       " 'rouge-2': {'r': 0.07458305457942253,\n",
       "  'p': 0.013203658763387869,\n",
       "  'f': 0.02056554092145283},\n",
       " 'rouge-l': {'r': 0.3074820343460368,\n",
       "  'p': 0.06778743921906497,\n",
       "  'f': 0.10340511227595182}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summary_list_Cosine, reference_list,avg=True) # Glove cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.3528981329028243,\n",
       "  'p': 0.08357784203772847,\n",
       "  'f': 0.12763933018016813},\n",
       " 'rouge-2': {'r': 0.06966936364793404,\n",
       "  'p': 0.010628152375986993,\n",
       "  'f': 0.017523173576019103},\n",
       " 'rouge-l': {'r': 0.31325314458319087,\n",
       "  'p': 0.073493102461619,\n",
       "  'f': 0.11282576019471463}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summary_list_TFIDF, reference_list,avg=True) # TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compression to do it in a fast fashion\n",
    "compression_opts = dict(method='zip', archive_name='TextRankOutputSample.csv')  # Create a compression method to efficiently export data\n",
    "tifu_dataset.to_csv('TextRankOutputSample.zip', index=True, compression=compression_opts) # Export the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
