{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt') # one time execution\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Word Vectors ##\n",
    "\n",
    "def create_word_embeddings(embedding_path):\n",
    "\n",
    "    word_embedding_dict = {}\n",
    "\n",
    "    f = open(embedding_path, encoding='utf-8')\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embedding_dict[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Word embeddings succesfully extracted!\")\n",
    "    \n",
    "    return(word_embedding_dict)\n",
    "\n",
    "# word_embeddings = create_word_embeddings('glove.6b/glove.6B.100d.txt')\n",
    "\n",
    "## Remove stopwords from sentence ##\n",
    "\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_single_doc(doc):\n",
    "    \n",
    "    sentences = [] # Create empty temporary list\n",
    "    \n",
    "    for s in doc['documents']:\n",
    "        sentences.append(sent_tokenize(s))\n",
    "    \n",
    "    sentences = [y for x in sentences for y in x] # flatten list\n",
    "    \n",
    "    sentences = list(pd.Series(sentences))\n",
    "    \n",
    "    # Replace newlines and beginning marks\n",
    "\n",
    "    clean_sentences = list(map(lambda x: x.replace('\\n','').replace('\"b','').replace('\\\\n','').replace(\"'b\",'').replace('b\"','').replace(\"b'\",''),sentences))\n",
    "    \n",
    "    # Replace URL's and beginning marks\n",
    "\n",
    "    clean_sentences = [re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])','URL',i) for i in clean_sentences]\n",
    "    \n",
    "    # Replace subreddit indicator\n",
    "    \n",
    "    clean_sentences = list(map(lambda x: x.replace('r/','subreddit '),clean_sentences))\n",
    "    \n",
    "    # Remove punctuation, numbers and special characters\n",
    "\n",
    "    clean_sentences = [re.sub(r\"[^a-zA-Z0-9]+\",\" \",i) for i in clean_sentences]\n",
    "\n",
    "    # Lowercase everything\n",
    "\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "    # remove stopwords from the sentences\n",
    "\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    return(sentences, clean_sentences)\n",
    "    \n",
    "# sentence_list, pre_processed_sentences = pre_process_single_doc(sample_post)\n",
    "# print(pre_processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Sentence Vectors ##\n",
    "\n",
    "def create_sentence_vectors(preprocessed_doc):\n",
    "\n",
    "    sentence_vectors = []\n",
    "\n",
    "    for i in preprocessed_doc:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    \n",
    "    return(sentence_vectors)\n",
    "\n",
    "# sentence_vector = create_sentence_vectors(pre_processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get longest common substring ## \n",
    "\n",
    "def calculate_lcs(string1, string2):\n",
    "    lcs = 0;\n",
    "    for a in range(len(string1)):\n",
    "         for b in range(len(string2)):\n",
    "            k = 0;\n",
    "            while ((a + k) < len(string1) and (b + k) < len(string2)\n",
    "        and string1[a + k] == string2[b + k]):\n",
    "                k = k + 1;\n",
    "\n",
    "            lcs = max(lcs, k);\n",
    "    return lcs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Similarity Matrix based on Cosine Similarity ##\n",
    "\n",
    "def create_similarity_matrix_cosine_vector(sen_vec,sen_list):\n",
    "\n",
    "    # similarity matrix\n",
    "\n",
    "    sim_mat = np.zeros([len(sen_list), len(sen_list)])\n",
    "\n",
    "    for i in range(len(sen_list)):\n",
    "        for j in range(len(sen_list)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sen_vec[i].reshape(1,100), sen_vec[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    return(sim_mat)\n",
    "    \n",
    "# similarity_matrix = create_similarity_matrix(sentence_vector,sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Similarity Matrix based on Cosine Similarity ##\n",
    "\n",
    "def create_similarity_matrix_TFIDF(sen_list):\n",
    "\n",
    "    # similarity matrix\n",
    "\n",
    "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "    tfidf = vect.fit_transform(sen_list)     \n",
    "    pairwise_similarity = tfidf * tfidf.T\n",
    "    sim_mat = pairwise_similarity.toarray()\n",
    "    np.fill_diagonal(sim_mat, 0)\n",
    "\n",
    "    return(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Similarity Matrix based on LCS Similarity ##\n",
    "\n",
    "def create_similarity_matrix_lcs(sen_list):\n",
    "\n",
    "    # similarity matrix\n",
    "\n",
    "    sim_mat = np.zeros([len(sen_list), len(sen_list)])\n",
    "\n",
    "    for i in range(len(sen_list)):\n",
    "        for j in range(len(sen_list)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = calculate_lcs(sen_list[i], sen_list[j])\n",
    "    \n",
    "    return(sim_mat)\n",
    "    \n",
    "# similarity_matrix = create_similarity_matrix(sentence_vector,sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Graph from Similarity Matrix ##\n",
    "\n",
    "def create_graph_and_rank(sim_mat,sen_list,plot_graph=False):\n",
    "    \n",
    "    # Create graph\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "\n",
    "    # Call Pagerank\n",
    "\n",
    "    scores = nx.pagerank_numpy(nx_graph)\n",
    "    \n",
    "    if plot_graph == True:\n",
    "        # Plot the graph\n",
    "        nx.draw(nx_graph)\n",
    "    \n",
    "    ## Rank Sentences Based on PageRank Algorithm #\n",
    "    \n",
    "    return(sorted(((scores[i],s) for i,s in enumerate(sen_list)), reverse=True))\n",
    "\n",
    "# ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract top sentences as the summary ##\n",
    "\n",
    "def generate_summary(sen_ranks, sen_list):\n",
    "\n",
    "    # Use 20% of the length of the original post\n",
    "\n",
    "    sum_size = math.ceil(int(0.2 * len(sen_list)))\n",
    "    if sum_size < 1:\n",
    "        sum_size = 1\n",
    "\n",
    "    order_dict = dict()\n",
    "\n",
    "    for i in range(sum_size):\n",
    "        order_dict.update({sen_ranks[i][1]: sen_list.index(sen_ranks[i][1])})\n",
    "\n",
    "    sorted_values = sorted(order_dict.values()) # Sort the values\n",
    "    sorted_dict = {}\n",
    "\n",
    "    for i in sorted_values:\n",
    "        for k in order_dict.keys():\n",
    "            if order_dict[k] == i:\n",
    "                sorted_dict[k] = order_dict[k]\n",
    "                break\n",
    "                \n",
    "    # Replace newlines and beginning marks\n",
    "\n",
    "    clean_sentences = list(map(lambda x: x.replace('\\n','').replace('\"b','').replace('\\\\n','').replace(\"'b\",'').replace('b\"','').replace(\"b'\",''),sorted_dict))\n",
    "    \n",
    "    # Replace URL's and beginning marks\n",
    "\n",
    "    clean_sentences = [re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])','URL',i) for i in clean_sentences]\n",
    "    \n",
    "    # Replace subreddit indicator\n",
    "    \n",
    "    clean_sentences = list(map(lambda x: x.replace('r/','subreddit '),clean_sentences))\n",
    "    \n",
    "    # Remove punctuation, numbers and special characters\n",
    "    \n",
    "    clean_sentences = [re.sub(r\"[^a-zA-Z0-9%',.-]+\",\" \",i) for i in clean_sentences]\n",
    "    \n",
    "    return(\" \".join(clean_sentences))\n",
    "\n",
    "# summary = generate_summary(ranking_scores,sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifu_dataset = pd.read_csv(\"out/out.csv\",index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42136 entries, 0 to 42135\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   documents     42136 non-null  object \n",
      " 1   num_comments  42136 non-null  int64  \n",
      " 2   score         42136 non-null  int64  \n",
      " 3   title         42136 non-null  object \n",
      " 4   tldr          42136 non-null  object \n",
      " 5   ups           42136 non-null  int64  \n",
      " 6   upvote_ratio  42136 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tifu_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>tldr</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"me and a friend decided to go to the beach l...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>b'liking seafood'</td>\n",
       "      <td>b'had delicious seafood. almost flooded a toil...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'obligatory this happened last thursday. \\n\\n...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>b'joking about being a whore'</td>\n",
       "      <td>b'made a joke about being a whore in class. ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'this was actually a few years ago (obligator...</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>b'burning my finger with molten starburst'</td>\n",
       "      <td>b'i burned my finger with a microwaved starbur...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'this is my first post on reddit, so i\\'m sor...</td>\n",
       "      <td>23</td>\n",
       "      <td>152</td>\n",
       "      <td>b'skipping my heart meds. nsfw'</td>\n",
       "      <td>b'a girl tried to give me a banana, smashed a ...</td>\n",
       "      <td>152</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"unlike most tifu this actually just happened...</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>b'overcoming my fears'</td>\n",
       "      <td>b'noticed a spider above my bed,decided to ove...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents  num_comments  score  \\\n",
       "0  b\"me and a friend decided to go to the beach l...             1      8   \n",
       "1  b'obligatory this happened last thursday. \\n\\n...             4      0   \n",
       "2  b'this was actually a few years ago (obligator...            24     25   \n",
       "3  b'this is my first post on reddit, so i\\'m sor...            23    152   \n",
       "4  b\"unlike most tifu this actually just happened...            10     21   \n",
       "\n",
       "                                        title  \\\n",
       "0                           b'liking seafood'   \n",
       "1               b'joking about being a whore'   \n",
       "2  b'burning my finger with molten starburst'   \n",
       "3             b'skipping my heart meds. nsfw'   \n",
       "4                      b'overcoming my fears'   \n",
       "\n",
       "                                                tldr  ups  upvote_ratio  \n",
       "0  b'had delicious seafood. almost flooded a toil...    8          0.76  \n",
       "1  b'made a joke about being a whore in class. ma...    0          0.50  \n",
       "2  b'i burned my finger with a microwaved starbur...   25          0.81  \n",
       "3  b'a girl tried to give me a banana, smashed a ...  152          0.90  \n",
       "4  b'noticed a spider above my bed,decided to ove...   21          0.84  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tifu_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings succesfully extracted!\n"
     ]
    }
   ],
   "source": [
    "# Execute once\n",
    "\n",
    "word_embeddings = create_word_embeddings('glove.6b/glove.6B.100d.txt')\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_textrank_summarization(doc,sim_measure): # Sim measure should be either cosine or lcs\n",
    "\n",
    "    if sim_measure == \"cosine\":\n",
    "        \n",
    "        sentence_list, pre_processed_sentences = pre_process_single_doc(doc)\n",
    "\n",
    "        sentence_vector = create_sentence_vectors(pre_processed_sentences)\n",
    "\n",
    "        similarity_matrix = create_similarity_matrix_cosine_vector(sentence_vector,sentence_list)\n",
    "\n",
    "        ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)\n",
    "\n",
    "        summary = generate_summary(ranking_scores,sentence_list)\n",
    "                \n",
    "    if sim_measure == \"lcs\":\n",
    "        \n",
    "        sentence_list, pre_processed_sentences = pre_process_single_doc(doc)\n",
    "        \n",
    "        similarity_matrix = create_similarity_matrix_lcs(pre_processed_sentences)\n",
    "\n",
    "        ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)\n",
    "\n",
    "        summary = generate_summary(ranking_scores,sentence_list)\n",
    "        \n",
    "    if sim_measure == \"tfidf\":\n",
    "        \n",
    "        sentence_list, pre_processed_sentences = pre_process_single_doc(doc)\n",
    "        \n",
    "        similarity_matrix = create_similarity_matrix_TFIDF(pre_processed_sentences)\n",
    "\n",
    "        ranking_scores = create_graph_and_rank(similarity_matrix,sentence_list)\n",
    "\n",
    "        summary = generate_summary(ranking_scores,sentence_list)\n",
    "                \n",
    "    return(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TEXT______________\n",
      "b'throwaway for obvious reasons. \\n\\nthis was back when i was in my early teens, 13-15 i don\\'t remember exactly. anyways, i was at sleep-away camp for two weeks and it was one of my first experiences away from home for an extended period of time, and for some reason i have a really bad time with taking a shit when i\\'m not at a toilet i\\'m familiar with (like the one at my house). maybe it was also due to the fact that campground toilets are gross, and outhouses even more so. i don\\'t know. anyways i didn\\'t take a shit for at least eight days, maybe more. \\n\\nat this point the struggle is so real. i\\'m starting to have the \"i really need to take a shit\" feeling but somehow i\\'m keeping it in. another day passes. i have somehow still managed to keep the shit inside me. my stomach and entire abdominal area is starting to look a little swollen, and it actually hurts to lie on my stomach. still managing to hold it in, god knows how. \\n\\nin the afternoon, the group decided to go for a hike. i feign being sick, partly because i actually do feel like shit (no pun intended?) and also because i feel like intense walking does not mix well with a shit-packed stomach. anyways, the group leaves, the assistant counselor stays behind with me. for the first time in days, i\\'m alone. \\n\\nthe shit game gets too strong. maybe it\\'s partly psychological - my brain knows i\\'m alone now - but i don\\'t know why...suddenly i really need to take a shit. i get up from my cot in a rush and attempt to waddle as fast as i can to the nearest outhouse. except it\\'s too far - at least a football field\\'s length away. there is no chance of me making it there in time. \\n\\ni feel something wet skidding down my leg. oh god, it\\'s the poop. it felt like a wet, warm, snake was crawling down my leg. except it kept going. jesus christ, when will it end? at this point i was about 15 feet behind the tents where we slept. \\n\\nwell, at this point there\\'s not much i can do and i\\'m super grossed out. so i run to the bathroom, wipe myself off, and run back to the tents, pretending like nothing happened. \\n\\ni suppose this is as good a time as ever to introduce the weird kid in our group - his name is harris. (obligatory name change). harris was the type of kid who would do something super creepy and when you asked him to stop, he didn\\'t get what you meant. perhaps he was somewhere on the autism spectrum, i don\\'t fucking know. he was a nice kid at heart, but...hard to get alone with due to his strangeness and tendency to be super annoying. he was also the type of kid who was probably the most likely to so something weird like take a shit in the woods behind the sleeping area...well, you see where this is going. \\n\\nfast forward a couple days later - my friends complain to the counselor about a terrible stench coming from the woods. they go investigate - lo and behold, a massive human log of shit. \\n\\nand guess who was blamed and sent home for being unsanitary and possibly mentally insane?'\n",
      "_____________TLDR______________\n",
      "b'shat in the woods, got slightly mental kid sent home'\n"
     ]
    }
   ],
   "source": [
    "sample_post = tifu_dataset[2459:2460] # Select one document\n",
    "\n",
    "text = sample_post['documents'][2459]\n",
    "print(\"_____________TEXT______________\")\n",
    "print(text)\n",
    "\n",
    "tldr = sample_post['tldr'][2459]\n",
    "print(\"_____________TLDR______________\")\n",
    "print(tldr)\n",
    "\n",
    "# 2019 has faulty TLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______ LCS method _______\n",
      "anyways, i was at sleep-away camp for two weeks and it was one of my first experiences away from home for an extended period of time, and for some reason i have a really bad time with taking a shit when i 'm not at a toilet i 'm familiar with like the one at my house . anyways i didn 't take a shit for at least eight days, maybe more. i 'm starting to have the i really need to take a shit feeling but somehow i 'm keeping it in. i feign being sick, partly because i actually do feel like shit no pun intended  and also because i feel like intense walking does not mix well with a shit-packed stomach. maybe it 's partly psychological - my brain knows i 'm alone now - but i don 't know why...suddenly i really need to take a shit. he was also the type of kid who was probably the most likely to so something weird like take a shit in the woods behind the sleeping area...well, you see where this is going.\n",
      "_______ cosine method _______\n",
      "anyways, i was at sleep-away camp for two weeks and it was one of my first experiences away from home for an extended period of time, and for some reason i have a really bad time with taking a shit when i 'm not at a toilet i 'm familiar with like the one at my house . i 'm starting to have the i really need to take a shit feeling but somehow i 'm keeping it in. maybe it 's partly psychological - my brain knows i 'm alone now - but i don 't know why...suddenly i really need to take a shit. harris was the type of kid who would do something super creepy and when you asked him to stop, he didn 't get what you meant. he was a nice kid at heart, but...hard to get alone with due to his strangeness and tendency to be super annoying. he was also the type of kid who was probably the most likely to so something weird like take a shit in the woods behind the sleeping area...well, you see where this is going. fast forward a couple days later - my friends complain to the counselor about a terrible stench coming from the woods.\n",
      "_______ TFIDF method _______\n",
      "anyways, i was at sleep-away camp for two weeks and it was one of my first experiences away from home for an extended period of time, and for some reason i have a really bad time with taking a shit when i 'm not at a toilet i 'm familiar with like the one at my house . anyways i didn 't take a shit for at least eight days, maybe more. and also because i feel like intense walking does not mix well with a shit-packed stomach. for the first time in days, i 'm alone. maybe it 's partly psychological - my brain knows i 'm alone now - but i don 't know why...suddenly i really need to take a shit. i suppose this is as good a time as ever to introduce the weird kid in our group - his name is harris. he was also the type of kid who was probably the most likely to so something weird like take a shit in the woods behind the sleeping area...well, you see where this is going.\n"
     ]
    }
   ],
   "source": [
    "print(\"_______ LCS method _______\")\n",
    "\n",
    "summary_lcs = pipeline_textrank_summarization(sample_post,\"lcs\")\n",
    "print(summary_lcs)\n",
    "\n",
    "print(\"_______ cosine method _______\")\n",
    "\n",
    "summary_cosine = pipeline_textrank_summarization(sample_post,\"cosine\")\n",
    "print(summary_cosine)\n",
    "\n",
    "print(\"_______ TFIDF method _______\")\n",
    "\n",
    "summary_tfidf = pipeline_textrank_summarization(sample_post,\"tfidf\")\n",
    "print(summary_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.3, 'p': 0.028037383177570093, 'f': 0.05128204971875233},\n",
       "  'rouge-2': {'r': 0.1111111111111111,\n",
       "   'p': 0.005847953216374269,\n",
       "   'f': 0.011111110161111193},\n",
       "  'rouge-l': {'r': 0.2, 'p': 0.018691588785046728, 'f': 0.03418803262473526}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "rouge.get_scores(summary_tfidf, tldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 42136/42136 [12:22:06<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_list_LCS = []\n",
    "summary_list_Cosine = []\n",
    "summary_list_TFIDF = []\n",
    "\n",
    "for index, row in tqdm(tifu_dataset.iterrows(), total=tifu_dataset.shape[0]):\n",
    "    curr_post = tifu_dataset[index:index+1] # Select one document\n",
    "    \n",
    "    try:\n",
    "        summary_list_Cosine.append(pipeline_textrank_summarization(curr_post,\"cosine\"))\n",
    "    except Exception:\n",
    "        summary_list_Cosine.append(\"error\")\n",
    "        \n",
    "    try:\n",
    "        summary_list_LCS.append(pipeline_textrank_summarization(curr_post,\"lcs\"))\n",
    "    except Exception:\n",
    "        summary_list_LCS.append(\"error\")\n",
    "                \n",
    "    try:\n",
    "        summary_list_TFIDF.append(pipeline_textrank_summarization(curr_post,\"tfidf\"))\n",
    "    except Exception:\n",
    "        summary_list_TFIDF.append(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_list = list(tifu_dataset['tldr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifu_dataset[\"LCS\"] = summary_list_LCS\n",
    "tifu_dataset[\"COSINE\"] = summary_list_Cosine\n",
    "tifu_dataset[\"TFIDF\"] = summary_list_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.34185330774196,\n",
       "  'p': 0.10462973284345654,\n",
       "  'f': 0.14846601011048802},\n",
       " 'rouge-2': {'r': 0.0684301096125939,\n",
       "  'p': 0.017297407796005713,\n",
       "  'f': 0.025020455502450897},\n",
       " 'rouge-l': {'r': 0.2943874568887026,\n",
       "  'p': 0.08777490071442022,\n",
       "  'f': 0.12528979208646296}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summary_list_LCS, reference_list,avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.3293588628008475,\n",
       "  'p': 0.09986659789070577,\n",
       "  'f': 0.14249724711983686},\n",
       " 'rouge-2': {'r': 0.06021175277383512,\n",
       "  'p': 0.015065330239948773,\n",
       "  'f': 0.021950238359046435},\n",
       " 'rouge-l': {'r': 0.28269857729130543,\n",
       "  'p': 0.08352918051864501,\n",
       "  'f': 0.11987308456461475}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summary_list_Cosine, reference_list,avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.31396949702276566,\n",
       "  'p': 0.1170709075381415,\n",
       "  'f': 0.15657601097597862},\n",
       " 'rouge-2': {'r': 0.06168557674975509,\n",
       "  'p': 0.019239124805426392,\n",
       "  'f': 0.026236436843573904},\n",
       " 'rouge-l': {'r': 0.2695735901166072,\n",
       "  'p': 0.09824747553032066,\n",
       "  'f': 0.13210622867212768}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(summary_list_TFIDF, reference_list,avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip', archive_name='TextRankOutput.csv')  # Create a compression method to efficiently export data\n",
    "tifu_dataset.to_csv('TextRankOutput.zip', index=True, compression=compression_opts) # Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42136 entries, 0 to 42135\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   documents     42136 non-null  object \n",
      " 1   num_comments  42136 non-null  int64  \n",
      " 2   score         42136 non-null  int64  \n",
      " 3   title         42136 non-null  object \n",
      " 4   tldr          42136 non-null  object \n",
      " 5   ups           42136 non-null  int64  \n",
      " 6   upvote_ratio  42136 non-null  float64\n",
      " 7   LCS           42136 non-null  object \n",
      " 8   COSINE        42136 non-null  object \n",
      " 9   TFIDF         42136 non-null  object \n",
      "dtypes: float64(1), int64(3), object(6)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "tifu_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
